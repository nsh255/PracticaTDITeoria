{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyOhW8Kj9jNAV1PRH+hVimaB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nsh255/PracticaTDITeoria/blob/main/PracticaTDITeorico.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Comienzo del programa en Python usando Google Collab\n",
        "\n"
      ],
      "metadata": {
        "id": "7ALqzD-DK2v3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import YolosImageProcessor, YolosForObjectDetection\n",
        "from PIL import Image\n",
        "import torch\n",
        "import requests\n",
        "\n",
        "url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n",
        "image = Image.open(requests.get(url, stream=True).raw)\n",
        "\n",
        "model = YolosForObjectDetection.from_pretrained('hustvl/yolos-tiny')\n",
        "image_processor = YolosImageProcessor.from_pretrained(\"hustvl/yolos-tiny\")\n",
        "\n",
        "inputs = image_processor(images=image, return_tensors=\"pt\")\n",
        "outputs = model(**inputs)\n",
        "\n",
        "# model predicts bounding boxes and corresponding COCO classes\n",
        "logits = outputs.logits\n",
        "bboxes = outputs.pred_boxes\n",
        "\n",
        "\n",
        "# print results\n",
        "target_sizes = torch.tensor([image.size[::-1]])\n",
        "results = image_processor.post_process_object_detection(outputs, threshold=0.9, target_sizes=target_sizes)[0]\n",
        "for score, label, box in zip(results[\"scores\"], results[\"labels\"], results[\"boxes\"]):\n",
        "    box = [round(i, 2) for i in box.tolist()]\n",
        "    print(\n",
        "        f\"Detected {model.config.id2label[label.item()]} with confidence \"\n",
        "        f\"{round(score.item(), 3)} at location {box}\"\n",
        "    )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dwbIRxj1KpO-",
        "outputId": "4aa79862-157c-46f1-c1a7-d06d3ad01c31"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Detected remote with confidence 0.991 at location [46.48, 72.78, 178.98, 119.3]\n",
            "Detected remote with confidence 0.908 at location [336.48, 79.27, 368.23, 192.36]\n",
            "Detected cat with confidence 0.934 at location [337.18, 18.06, 638.14, 373.09]\n",
            "Detected cat with confidence 0.979 at location [10.93, 53.74, 313.41, 470.67]\n",
            "Detected remote with confidence 0.974 at location [41.63, 72.23, 178.09, 119.99]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Ahora prueba con mis imagenes propias"
      ],
      "metadata": {
        "id": "VYNaMnH9M9C4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import YolosImageProcessor, YolosForObjectDetection\n",
        "from PIL import Image\n",
        "import torch\n",
        "\n",
        "# Ruta de la imagen en tu equipo\n",
        "image_path = \"/content/image.jpg\"\n",
        "\n",
        "# Cargar la imagen\n",
        "image = Image.open(image_path)\n",
        "\n",
        "model = YolosForObjectDetection.from_pretrained('hustvl/yolos-tiny')\n",
        "image_processor = YolosImageProcessor.from_pretrained(\"hustvl/yolos-tiny\")\n",
        "\n",
        "inputs = image_processor(images=image, return_tensors=\"pt\")\n",
        "outputs = model(**inputs)\n",
        "\n",
        "# El modelo predice las cajas delimitadoras y las clases COCO correspondientes\n",
        "logits = outputs.logits\n",
        "bboxes = outputs.pred_boxes\n",
        "\n",
        "# Imprimir los resultados\n",
        "target_sizes = torch.tensor([image.size[::-1]])\n",
        "results = image_processor.post_process_object_detection(outputs, threshold=0.9, target_sizes=target_sizes)[0]\n",
        "for score, label, box in zip(results[\"scores\"], results[\"labels\"], results[\"boxes\"]):\n",
        "    box = [round(i, 2) for i in box.tolist()]\n",
        "    print(\n",
        "        f\"Detectado {model.config.id2label[label.item()]} con confianza \"\n",
        "        f\"{round(score.item(), 3)} en la ubicación {box}\"\n",
        "    )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bznWw51dNAIb",
        "outputId": "3c859bff-8e41-4a3a-86a9-9ecfaab869fa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Detectado person con confianza 0.934 en la ubicación [941.66, 83.32, 1049.8, 206.1]\n",
            "Detectado person con confianza 0.991 en la ubicación [1365.47, 350.59, 1505.84, 483.73]\n",
            "Detectado person con confianza 0.99 en la ubicación [827.1, 343.34, 960.43, 546.18]\n",
            "Detectado person con confianza 0.92 en la ubicación [749.14, 300.27, 902.38, 458.5]\n",
            "Detectado person con confianza 0.991 en la ubicación [1502.86, 536.9, 1672.19, 694.0]\n",
            "Detectado tennis racket con confianza 0.905 en la ubicación [827.7, 327.73, 922.05, 376.85]\n",
            "Detectado person con confianza 0.982 en la ubicación [1017.28, 623.21, 1162.3, 811.92]\n",
            "Detectado tennis racket con confianza 0.907 en la ubicación [868.57, 312.85, 947.01, 341.22]\n",
            "Detectado person con confianza 0.973 en la ubicación [1032.15, 302.77, 1147.9, 413.98]\n",
            "Detectado person con confianza 0.923 en la ubicación [1029.14, 457.77, 1148.69, 593.32]\n",
            "Detectado person con confianza 0.992 en la ubicación [90.79, 581.99, 335.0, 787.46]\n",
            "Detectado sports ball con confianza 0.923 en la ubicación [720.73, 191.0, 752.45, 200.65]\n",
            "Detectado person con confianza 0.966 en la ubicación [1607.27, 364.65, 1763.43, 549.03]\n",
            "Detectado person con confianza 0.996 en la ubicación [580.86, 253.59, 710.72, 367.83]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# no me acuerdo"
      ],
      "metadata": {
        "id": "2ZkZnHI-hCtZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import YolosImageProcessor, YolosForObjectDetection\n",
        "from PIL import Image, ImageDraw\n",
        "import torch\n",
        "\n",
        "# Ruta de la imagen en tu equipo\n",
        "image_path = \"/content/image.jpg\"\n",
        "\n",
        "# Cargar la imagen\n",
        "image = Image.open(image_path)\n",
        "\n",
        "model = YolosForObjectDetection.from_pretrained('hustvl/yolos-tiny')\n",
        "image_processor = YolosImageProcessor.from_pretrained(\"hustvl/yolos-tiny\")\n",
        "\n",
        "inputs = image_processor(images=image, return_tensors=\"pt\")\n",
        "outputs = model(**inputs)\n",
        "\n",
        "# El modelo predice las cajas delimitadoras y las clases COCO correspondientes\n",
        "logits = outputs.logits\n",
        "bboxes = outputs.pred_boxes\n",
        "\n",
        "# Imprimir los resultados\n",
        "target_sizes = torch.tensor([image.size[::-1]])\n",
        "results = image_processor.post_process_object_detection(outputs, threshold=0.9, target_sizes=target_sizes)[0]\n",
        "for score, label, box in zip(results[\"scores\"], results[\"labels\"], results[\"boxes\"]):\n",
        "    box = [round(i, 2) for i in box.tolist()]\n",
        "    print(\n",
        "        f\"Detectado {model.config.id2label[label.item()]} con confianza \"\n",
        "        f\"{round(score.item(), 3)} en la ubicación {box}\"\n",
        "    )\n",
        "\n",
        "# Guardar las imágenes con las cajas delimitadoras\n",
        "for i, (score, label, box) in enumerate(zip(results[\"scores\"], results[\"labels\"], results[\"boxes\"])):\n",
        "    # Crear una copia de la imagen original\n",
        "    image_with_bbox = image.copy()\n",
        "\n",
        "    # Crear un objeto ImageDraw\n",
        "    draw = ImageDraw.Draw(image_with_bbox)\n",
        "\n",
        "    # Dibujar la caja delimitadora en la imagen\n",
        "    image_with_bbox = image_with_bbox.copy()\n",
        "    draw = ImageDraw.Draw(image_with_bbox)\n",
        "    draw.rectangle(tuple(box), outline=\"red\", width=2)\n",
        "\n",
        "    # Guardar la imagen con la caja delimitadora\n",
        "    image_with_bbox.save(f\"/content/imagen_{i+1}_con_bbox.jpg\")\n",
        "\n",
        "# Guardar la imagen original\n",
        "image.save(\"/content/image.jpg\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VQ63cWqYbdeM",
        "outputId": "62071ce6-d87f-48ca-da22-81f902417477"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Detectado person con confianza 0.937 en la ubicación [941.4, 83.67, 1050.47, 205.73]\n",
            "Detectado person con confianza 0.991 en la ubicación [1364.95, 350.22, 1505.33, 482.15]\n",
            "Detectado person con confianza 0.989 en la ubicación [827.56, 343.11, 960.86, 546.04]\n",
            "Detectado person con confianza 0.913 en la ubicación [749.2, 300.14, 901.84, 457.58]\n",
            "Detectado person con confianza 0.991 en la ubicación [1502.83, 537.16, 1671.41, 693.37]\n",
            "Detectado tennis racket con confianza 0.909 en la ubicación [829.74, 324.14, 924.51, 374.67]\n",
            "Detectado person con confianza 0.982 en la ubicación [1017.26, 623.97, 1161.9, 812.1]\n",
            "Detectado tennis racket con confianza 0.919 en la ubicación [870.26, 313.38, 948.99, 341.8]\n",
            "Detectado person con confianza 0.971 en la ubicación [1032.0, 303.04, 1147.4, 415.21]\n",
            "Detectado person con confianza 0.923 en la ubicación [1030.26, 459.35, 1147.4, 593.15]\n",
            "Detectado person con confianza 0.992 en la ubicación [91.66, 582.31, 334.27, 787.44]\n",
            "Detectado sports ball con confianza 0.918 en la ubicación [720.27, 190.99, 751.87, 200.57]\n",
            "Detectado person con confianza 0.956 en la ubicación [1608.32, 367.16, 1764.43, 549.72]\n",
            "Detectado person con confianza 0.995 en la ubicación [580.49, 253.78, 711.42, 368.19]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# DIOS DI"
      ],
      "metadata": {
        "id": "I610BAdbhIVr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import YolosImageProcessor, YolosForObjectDetection\n",
        "from PIL import Image, ImageDraw\n",
        "import torch\n",
        "\n",
        "# Ruta de la imagen en tu equipo\n",
        "image_path = \"/content/image.jpg\"\n",
        "\n",
        "# Cargar la imagen\n",
        "image = Image.open(image_path)\n",
        "\n",
        "model = YolosForObjectDetection.from_pretrained('hustvl/yolos-tiny')\n",
        "image_processor = YolosImageProcessor.from_pretrained(\"hustvl/yolos-tiny\")\n",
        "\n",
        "inputs = image_processor(images=image, return_tensors=\"pt\")\n",
        "outputs = model(**inputs)\n",
        "\n",
        "# El modelo predice las cajas delimitadoras y las clases COCO correspondientes\n",
        "logits = outputs.logits\n",
        "bboxes = outputs.pred_boxes\n",
        "\n",
        "# Imprimir los resultados\n",
        "target_sizes = torch.tensor([image.size[::-1]])\n",
        "results = image_processor.post_process_object_detection(outputs, threshold=0.85, target_sizes=target_sizes)[0]\n",
        "\n",
        "# Crear una copia de la imagen original\n",
        "image_with_bbox = image.copy()\n",
        "\n",
        "# Crear un objeto ImageDraw\n",
        "draw = ImageDraw.Draw(image_with_bbox)\n",
        "\n",
        "# Imprimir los resultados\n",
        "target_sizes = torch.tensor([image.size[::-1]])\n",
        "results = image_processor.post_process_object_detection(outputs, threshold=0.9, target_sizes=target_sizes)[0]\n",
        "for score, label, box in zip(results[\"scores\"], results[\"labels\"], results[\"boxes\"]):\n",
        "    box = [round(i, 2) for i in box.tolist()]\n",
        "    print(\n",
        "        f\"Detectado {model.config.id2label[label.item()]} con confianza \"\n",
        "        f\"{round(score.item(), 3)} en la ubicación {box}\"\n",
        "    )\n",
        "\n",
        "# Dibujar las cajas delimitadoras en la imagen\n",
        "for score, label, box in zip(results[\"scores\"], results[\"labels\"], results[\"boxes\"]):\n",
        "    box = [round(i, 2) for i in box.tolist()]\n",
        "    draw.rectangle(tuple(box), outline=\"red\", width=2)\n",
        "\n",
        "# Guardar la imagen con todas las cajas delimitadoras\n",
        "image_with_bbox.save(\"/content/image_with_bbox.jpg\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KwxLIYQlhKmI",
        "outputId": "704ca911-8e51-4d83-c4cb-5b92a8e657a7"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Detectado person con confianza 0.937 en la ubicación [941.35, 83.68, 1050.52, 205.8]\n",
            "Detectado person con confianza 0.991 en la ubicación [1364.94, 350.23, 1505.4, 482.17]\n",
            "Detectado person con confianza 0.989 en la ubicación [827.52, 343.46, 960.96, 546.04]\n",
            "Detectado person con confianza 0.913 en la ubicación [749.13, 300.19, 902.04, 457.57]\n",
            "Detectado person con confianza 0.991 en la ubicación [1502.82, 537.17, 1671.49, 693.3]\n",
            "Detectado tennis racket con confianza 0.909 en la ubicación [829.63, 324.38, 924.55, 374.88]\n",
            "Detectado person con confianza 0.982 en la ubicación [1017.22, 623.97, 1161.93, 812.1]\n",
            "Detectado tennis racket con confianza 0.919 en la ubicación [870.02, 313.16, 947.99, 341.09]\n",
            "Detectado person con confianza 0.971 en la ubicación [1031.92, 303.07, 1147.48, 415.28]\n",
            "Detectado person con confianza 0.922 en la ubicación [1030.25, 459.42, 1147.45, 593.1]\n",
            "Detectado person con confianza 0.992 en la ubicación [91.61, 582.32, 334.35, 787.28]\n",
            "Detectado sports ball con confianza 0.918 en la ubicación [720.2, 190.95, 751.91, 200.56]\n",
            "Detectado person con confianza 0.955 en la ubicación [1608.29, 367.22, 1764.52, 549.66]\n",
            "Detectado person con confianza 0.995 en la ubicación [580.43, 253.78, 711.46, 368.1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Ahora vamos a probar a pillar solo personas"
      ],
      "metadata": {
        "id": "yIkiM3Yukq1i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import YolosImageProcessor, YolosForObjectDetection\n",
        "from PIL import Image, ImageDraw\n",
        "import torch\n",
        "\n",
        "# Ruta de la imagen en tu equipo\n",
        "image_path = \"/content/image.jpg\"\n",
        "\n",
        "# Cargar la imagen\n",
        "image = Image.open(image_path)\n",
        "\n",
        "model = YolosForObjectDetection.from_pretrained('hustvl/yolos-tiny')\n",
        "image_processor = YolosImageProcessor.from_pretrained(\"hustvl/yolos-tiny\")\n",
        "\n",
        "inputs = image_processor(images=image, return_tensors=\"pt\")\n",
        "outputs = model(**inputs)\n",
        "\n",
        "# El modelo predice las cajas delimitadoras y las clases COCO correspondientes\n",
        "logits = outputs.logits\n",
        "bboxes = outputs.pred_boxes\n",
        "\n",
        "# Imprimir los resultados\n",
        "target_sizes = torch.tensor([image.size[::-1]])\n",
        "results = image_processor.post_process_object_detection(outputs, threshold=0.85, target_sizes=target_sizes)[0]\n",
        "\n",
        "# Crear una copia de la imagen original\n",
        "image_with_bbox = image.copy()\n",
        "\n",
        "# Crear un objeto ImageDraw\n",
        "draw = ImageDraw.Draw(image_with_bbox)\n",
        "\n",
        "# Dibujar las cajas delimitadoras solo para las etiquetas \"person\"\n",
        "for score, label, box in zip(results[\"scores\"], results[\"labels\"], results[\"boxes\"]):\n",
        "    if model.config.id2label[label.item()] == \"person\":\n",
        "        box = [round(i, 2) for i in box.tolist()]\n",
        "        draw.rectangle(tuple(box), outline=\"red\", width=2)\n",
        "\n",
        "# Guardar la imagen con todas las cajas delimitadoras\n",
        "image_with_bbox.save(\"/content/image_with_bbox_person.jpg\")"
      ],
      "metadata": {
        "id": "o79C7Uo8k2YK"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Ahora con video"
      ],
      "metadata": {
        "id": "s30F_jPyrfK_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "from transformers import YolosImageProcessor, YolosForObjectDetection\n",
        "from PIL import Image, ImageDraw\n",
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "# Ruta del video en tu equipo\n",
        "video_path = \"/content/pruebas.mp4\"\n",
        "\n",
        "# Cargar el modelo y el procesador de imágenes\n",
        "model = YolosForObjectDetection.from_pretrained('hustvl/yolos-tiny')\n",
        "image_processor = YolosImageProcessor.from_pretrained(\"hustvl/yolos-tiny\")\n",
        "\n",
        "# Crear el objeto VideoCapture para leer el video\n",
        "cap = cv2.VideoCapture(video_path)\n",
        "\n",
        "# Obtener el ancho y alto del video\n",
        "width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "\n",
        "# Crear el objeto VideoWriter para guardar el video con las detecciones\n",
        "output_path = \"/content/salida.mp4\"\n",
        "fourcc = cv2.VideoWriter_fourcc(*\"mp4v\")\n",
        "out = cv2.VideoWriter(output_path, fourcc, 30.0, (width, height))\n",
        "\n",
        "while cap.isOpened():\n",
        "    # Leer el siguiente frame del video\n",
        "    ret, frame = cap.read()\n",
        "\n",
        "    if not ret:\n",
        "        break\n",
        "\n",
        "    # Convertir el frame a formato PIL Image\n",
        "    image = Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
        "\n",
        "    # Procesar la imagen con el modelo de detección de objetos\n",
        "    inputs = image_processor(images=image, return_tensors=\"pt\")\n",
        "    outputs = model(**inputs)\n",
        "\n",
        "    # Obtener las detecciones de personas\n",
        "    target_sizes = torch.tensor([image.size[::-1]])\n",
        "    results = image_processor.post_process_object_detection(outputs, threshold=0.85, target_sizes=target_sizes)[0]\n",
        "\n",
        "    # Dibujar las cajas delimitadoras en el frame\n",
        "    draw = ImageDraw.Draw(image)\n",
        "    for score, label, box in zip(results[\"scores\"], results[\"labels\"], results[\"boxes\"]):\n",
        "        if model.config.id2label[label.item()] == \"person\":\n",
        "            box = [round(i, 2) for i in box.tolist()]\n",
        "            draw.rectangle(tuple(box), outline=\"red\", width=2)\n",
        "\n",
        "    # Guardar el frame con las detecciones en el video de salida\n",
        "    out.write(cv2.cvtColor(np.array(image), cv2.COLOR_RGB2BGR))\n",
        "\n",
        "# Liberar los recursos\n",
        "cap.release()\n",
        "out.release()\n",
        "cv2.destroyAllWindows()"
      ],
      "metadata": {
        "id": "6_ezbvGortqX"
      },
      "execution_count": 23,
      "outputs": []
    }
  ]
}