{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nsh255/PracticaTDITeoria/blob/main/PracticaTDITeorico.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Comienzo del programa en Python usando Google Collab\n",
        "\n"
      ],
      "metadata": {
        "id": "7ALqzD-DK2v3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import YolosImageProcessor, YolosForObjectDetection\n",
        "from PIL import Image\n",
        "import torch\n",
        "import requests\n",
        "\n",
        "url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n",
        "image = Image.open(requests.get(url, stream=True).raw)\n",
        "\n",
        "model = YolosForObjectDetection.from_pretrained('hustvl/yolos-tiny')\n",
        "image_processor = YolosImageProcessor.from_pretrained(\"hustvl/yolos-tiny\")\n",
        "\n",
        "inputs = image_processor(images=image, return_tensors=\"pt\")\n",
        "outputs = model(**inputs)\n",
        "\n",
        "# model predicts bounding boxes and corresponding COCO classes\n",
        "logits = outputs.logits\n",
        "bboxes = outputs.pred_boxes\n",
        "\n",
        "\n",
        "# print results\n",
        "target_sizes = torch.tensor([image.size[::-1]])\n",
        "results = image_processor.post_process_object_detection(outputs, threshold=0.9, target_sizes=target_sizes)[0]\n",
        "for score, label, box in zip(results[\"scores\"], results[\"labels\"], results[\"boxes\"]):\n",
        "    box = [round(i, 2) for i in box.tolist()]\n",
        "    print(\n",
        "        f\"Detected {model.config.id2label[label.item()]} with confidence \"\n",
        "        f\"{round(score.item(), 3)} at location {box}\"\n",
        "    )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dwbIRxj1KpO-",
        "outputId": "4aa79862-157c-46f1-c1a7-d06d3ad01c31"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Detected remote with confidence 0.991 at location [46.48, 72.78, 178.98, 119.3]\n",
            "Detected remote with confidence 0.908 at location [336.48, 79.27, 368.23, 192.36]\n",
            "Detected cat with confidence 0.934 at location [337.18, 18.06, 638.14, 373.09]\n",
            "Detected cat with confidence 0.979 at location [10.93, 53.74, 313.41, 470.67]\n",
            "Detected remote with confidence 0.974 at location [41.63, 72.23, 178.09, 119.99]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Ahora prueba con mis imagenes propias"
      ],
      "metadata": {
        "id": "VYNaMnH9M9C4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import YolosImageProcessor, YolosForObjectDetection\n",
        "from PIL import Image\n",
        "import torch\n",
        "\n",
        "# Ruta de la imagen en tu equipo\n",
        "image_path = \"/content/image.jpg\"\n",
        "\n",
        "# Cargar la imagen\n",
        "image = Image.open(image_path)\n",
        "\n",
        "model = YolosForObjectDetection.from_pretrained('hustvl/yolos-tiny')\n",
        "image_processor = YolosImageProcessor.from_pretrained(\"hustvl/yolos-tiny\")\n",
        "\n",
        "inputs = image_processor(images=image, return_tensors=\"pt\")\n",
        "outputs = model(**inputs)\n",
        "\n",
        "# El modelo predice las cajas delimitadoras y las clases COCO correspondientes\n",
        "logits = outputs.logits\n",
        "bboxes = outputs.pred_boxes\n",
        "\n",
        "# Imprimir los resultados\n",
        "target_sizes = torch.tensor([image.size[::-1]])\n",
        "results = image_processor.post_process_object_detection(outputs, threshold=0.9, target_sizes=target_sizes)[0]\n",
        "for score, label, box in zip(results[\"scores\"], results[\"labels\"], results[\"boxes\"]):\n",
        "    box = [round(i, 2) for i in box.tolist()]\n",
        "    print(\n",
        "        f\"Detectado {model.config.id2label[label.item()]} con confianza \"\n",
        "        f\"{round(score.item(), 3)} en la ubicación {box}\"\n",
        "    )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bznWw51dNAIb",
        "outputId": "3c859bff-8e41-4a3a-86a9-9ecfaab869fa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Detectado person con confianza 0.934 en la ubicación [941.66, 83.32, 1049.8, 206.1]\n",
            "Detectado person con confianza 0.991 en la ubicación [1365.47, 350.59, 1505.84, 483.73]\n",
            "Detectado person con confianza 0.99 en la ubicación [827.1, 343.34, 960.43, 546.18]\n",
            "Detectado person con confianza 0.92 en la ubicación [749.14, 300.27, 902.38, 458.5]\n",
            "Detectado person con confianza 0.991 en la ubicación [1502.86, 536.9, 1672.19, 694.0]\n",
            "Detectado tennis racket con confianza 0.905 en la ubicación [827.7, 327.73, 922.05, 376.85]\n",
            "Detectado person con confianza 0.982 en la ubicación [1017.28, 623.21, 1162.3, 811.92]\n",
            "Detectado tennis racket con confianza 0.907 en la ubicación [868.57, 312.85, 947.01, 341.22]\n",
            "Detectado person con confianza 0.973 en la ubicación [1032.15, 302.77, 1147.9, 413.98]\n",
            "Detectado person con confianza 0.923 en la ubicación [1029.14, 457.77, 1148.69, 593.32]\n",
            "Detectado person con confianza 0.992 en la ubicación [90.79, 581.99, 335.0, 787.46]\n",
            "Detectado sports ball con confianza 0.923 en la ubicación [720.73, 191.0, 752.45, 200.65]\n",
            "Detectado person con confianza 0.966 en la ubicación [1607.27, 364.65, 1763.43, 549.03]\n",
            "Detectado person con confianza 0.996 en la ubicación [580.86, 253.59, 710.72, 367.83]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Ahora dibujamos cuadrados alrededor de las cosas detectadas\n",
        "No ha dibujado correctamente por librerias"
      ],
      "metadata": {
        "id": "2ZkZnHI-hCtZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import YolosImageProcessor, YolosForObjectDetection\n",
        "from PIL import Image, ImageDraw\n",
        "import torch\n",
        "\n",
        "# Ruta de la imagen en tu equipo\n",
        "image_path = \"/content/image.jpg\"\n",
        "\n",
        "# Cargar la imagen\n",
        "image = Image.open(image_path)\n",
        "\n",
        "model = YolosForObjectDetection.from_pretrained('hustvl/yolos-tiny')\n",
        "image_processor = YolosImageProcessor.from_pretrained(\"hustvl/yolos-tiny\")\n",
        "\n",
        "inputs = image_processor(images=image, return_tensors=\"pt\")\n",
        "outputs = model(**inputs)\n",
        "\n",
        "# El modelo predice las cajas delimitadoras y las clases COCO correspondientes\n",
        "logits = outputs.logits\n",
        "bboxes = outputs.pred_boxes\n",
        "\n",
        "# Imprimir los resultados\n",
        "target_sizes = torch.tensor([image.size[::-1]])\n",
        "results = image_processor.post_process_object_detection(outputs, threshold=0.9, target_sizes=target_sizes)[0]\n",
        "for score, label, box in zip(results[\"scores\"], results[\"labels\"], results[\"boxes\"]):\n",
        "    box = [round(i, 2) for i in box.tolist()]\n",
        "    print(\n",
        "        f\"Detectado {model.config.id2label[label.item()]} con confianza \"\n",
        "        f\"{round(score.item(), 3)} en la ubicación {box}\"\n",
        "    )\n",
        "\n",
        "# Guardar las imágenes con las cajas delimitadoras\n",
        "for i, (score, label, box) in enumerate(zip(results[\"scores\"], results[\"labels\"], results[\"boxes\"])):\n",
        "    # Crear una copia de la imagen original\n",
        "    image_with_bbox = image.copy()\n",
        "\n",
        "    # Crear un objeto ImageDraw\n",
        "    draw = ImageDraw.Draw(image_with_bbox)\n",
        "\n",
        "    # Dibujar la caja delimitadora en la imagen\n",
        "    image_with_bbox = image_with_bbox.copy()\n",
        "    draw = ImageDraw.Draw(image_with_bbox)\n",
        "    draw.rectangle(tuple(box), outline=\"red\", width=2)\n",
        "\n",
        "    # Guardar la imagen con la caja delimitadora\n",
        "    image_with_bbox.save(f\"/content/imagen_{i+1}_con_bbox.jpg\")\n",
        "\n",
        "# Guardar la imagen original\n",
        "image.save(\"/content/image.jpg\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VQ63cWqYbdeM",
        "outputId": "62071ce6-d87f-48ca-da22-81f902417477"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Detectado person con confianza 0.937 en la ubicación [941.4, 83.67, 1050.47, 205.73]\n",
            "Detectado person con confianza 0.991 en la ubicación [1364.95, 350.22, 1505.33, 482.15]\n",
            "Detectado person con confianza 0.989 en la ubicación [827.56, 343.11, 960.86, 546.04]\n",
            "Detectado person con confianza 0.913 en la ubicación [749.2, 300.14, 901.84, 457.58]\n",
            "Detectado person con confianza 0.991 en la ubicación [1502.83, 537.16, 1671.41, 693.37]\n",
            "Detectado tennis racket con confianza 0.909 en la ubicación [829.74, 324.14, 924.51, 374.67]\n",
            "Detectado person con confianza 0.982 en la ubicación [1017.26, 623.97, 1161.9, 812.1]\n",
            "Detectado tennis racket con confianza 0.919 en la ubicación [870.26, 313.38, 948.99, 341.8]\n",
            "Detectado person con confianza 0.971 en la ubicación [1032.0, 303.04, 1147.4, 415.21]\n",
            "Detectado person con confianza 0.923 en la ubicación [1030.26, 459.35, 1147.4, 593.15]\n",
            "Detectado person con confianza 0.992 en la ubicación [91.66, 582.31, 334.27, 787.44]\n",
            "Detectado sports ball con confianza 0.918 en la ubicación [720.27, 190.99, 751.87, 200.57]\n",
            "Detectado person con confianza 0.956 en la ubicación [1608.32, 367.16, 1764.43, 549.72]\n",
            "Detectado person con confianza 0.995 en la ubicación [580.49, 253.78, 711.42, 368.19]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Volvemos a intentarlo pero con otra forma"
      ],
      "metadata": {
        "id": "I610BAdbhIVr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import YolosImageProcessor, YolosForObjectDetection\n",
        "from PIL import Image, ImageDraw\n",
        "import torch\n",
        "\n",
        "# Ruta de la imagen en tu equipo\n",
        "image_path = \"/content/image.jpg\"\n",
        "\n",
        "# Cargar la imagen\n",
        "image = Image.open(image_path)\n",
        "\n",
        "model = YolosForObjectDetection.from_pretrained('hustvl/yolos-tiny')\n",
        "image_processor = YolosImageProcessor.from_pretrained(\"hustvl/yolos-tiny\")\n",
        "\n",
        "inputs = image_processor(images=image, return_tensors=\"pt\")\n",
        "outputs = model(**inputs)\n",
        "\n",
        "# El modelo predice las cajas delimitadoras y las clases COCO correspondientes\n",
        "logits = outputs.logits\n",
        "bboxes = outputs.pred_boxes\n",
        "\n",
        "# Imprimir los resultados\n",
        "target_sizes = torch.tensor([image.size[::-1]])\n",
        "results = image_processor.post_process_object_detection(outputs, threshold=0.85, target_sizes=target_sizes)[0]\n",
        "\n",
        "# Crear una copia de la imagen original\n",
        "image_with_bbox = image.copy()\n",
        "\n",
        "# Crear un objeto ImageDraw\n",
        "draw = ImageDraw.Draw(image_with_bbox)\n",
        "\n",
        "# Imprimir los resultados\n",
        "target_sizes = torch.tensor([image.size[::-1]])\n",
        "results = image_processor.post_process_object_detection(outputs, threshold=0.9, target_sizes=target_sizes)[0]\n",
        "for score, label, box in zip(results[\"scores\"], results[\"labels\"], results[\"boxes\"]):\n",
        "    box = [round(i, 2) for i in box.tolist()]\n",
        "    print(\n",
        "        f\"Detectado {model.config.id2label[label.item()]} con confianza \"\n",
        "        f\"{round(score.item(), 3)} en la ubicación {box}\"\n",
        "    )\n",
        "\n",
        "# Dibujar las cajas delimitadoras en la imagen\n",
        "for score, label, box in zip(results[\"scores\"], results[\"labels\"], results[\"boxes\"]):\n",
        "    box = [round(i, 2) for i in box.tolist()]\n",
        "    draw.rectangle(tuple(box), outline=\"red\", width=2)\n",
        "\n",
        "# Guardar la imagen con todas las cajas delimitadoras\n",
        "image_with_bbox.save(\"/content/image_with_bbox.jpg\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KwxLIYQlhKmI",
        "outputId": "704ca911-8e51-4d83-c4cb-5b92a8e657a7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Detectado person con confianza 0.937 en la ubicación [941.35, 83.68, 1050.52, 205.8]\n",
            "Detectado person con confianza 0.991 en la ubicación [1364.94, 350.23, 1505.4, 482.17]\n",
            "Detectado person con confianza 0.989 en la ubicación [827.52, 343.46, 960.96, 546.04]\n",
            "Detectado person con confianza 0.913 en la ubicación [749.13, 300.19, 902.04, 457.57]\n",
            "Detectado person con confianza 0.991 en la ubicación [1502.82, 537.17, 1671.49, 693.3]\n",
            "Detectado tennis racket con confianza 0.909 en la ubicación [829.63, 324.38, 924.55, 374.88]\n",
            "Detectado person con confianza 0.982 en la ubicación [1017.22, 623.97, 1161.93, 812.1]\n",
            "Detectado tennis racket con confianza 0.919 en la ubicación [870.02, 313.16, 947.99, 341.09]\n",
            "Detectado person con confianza 0.971 en la ubicación [1031.92, 303.07, 1147.48, 415.28]\n",
            "Detectado person con confianza 0.922 en la ubicación [1030.25, 459.42, 1147.45, 593.1]\n",
            "Detectado person con confianza 0.992 en la ubicación [91.61, 582.32, 334.35, 787.28]\n",
            "Detectado sports ball con confianza 0.918 en la ubicación [720.2, 190.95, 751.91, 200.56]\n",
            "Detectado person con confianza 0.955 en la ubicación [1608.29, 367.22, 1764.52, 549.66]\n",
            "Detectado person con confianza 0.995 en la ubicación [580.43, 253.78, 711.46, 368.1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Ahora vamos a probar a pillar solo personas"
      ],
      "metadata": {
        "id": "yIkiM3Yukq1i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import YolosImageProcessor, YolosForObjectDetection\n",
        "from PIL import Image, ImageDraw\n",
        "import torch\n",
        "\n",
        "# Ruta de la imagen en tu equipo\n",
        "image_path = \"/content/image.jpg\"\n",
        "\n",
        "# Cargar la imagen\n",
        "image = Image.open(image_path)\n",
        "\n",
        "model = YolosForObjectDetection.from_pretrained('hustvl/yolos-tiny')\n",
        "image_processor = YolosImageProcessor.from_pretrained(\"hustvl/yolos-tiny\")\n",
        "\n",
        "inputs = image_processor(images=image, return_tensors=\"pt\")\n",
        "outputs = model(**inputs)\n",
        "\n",
        "# El modelo predice las cajas delimitadoras y las clases COCO correspondientes\n",
        "logits = outputs.logits\n",
        "bboxes = outputs.pred_boxes\n",
        "\n",
        "# Imprimir los resultados\n",
        "target_sizes = torch.tensor([image.size[::-1]])\n",
        "results = image_processor.post_process_object_detection(outputs, threshold=0.85, target_sizes=target_sizes)[0]\n",
        "\n",
        "# Crear una copia de la imagen original\n",
        "image_with_bbox = image.copy()\n",
        "\n",
        "# Crear un objeto ImageDraw\n",
        "draw = ImageDraw.Draw(image_with_bbox)\n",
        "\n",
        "# Dibujar las cajas delimitadoras solo para las etiquetas \"person\"\n",
        "for score, label, box in zip(results[\"scores\"], results[\"labels\"], results[\"boxes\"]):\n",
        "    if model.config.id2label[label.item()] == \"person\":\n",
        "        box = [round(i, 2) for i in box.tolist()]\n",
        "        draw.rectangle(tuple(box), outline=\"red\", width=2)\n",
        "\n",
        "# Guardar la imagen con todas las cajas delimitadoras\n",
        "image_with_bbox.save(\"/content/image_with_bbox_person.jpg\")"
      ],
      "metadata": {
        "id": "o79C7Uo8k2YK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Ahora con video"
      ],
      "metadata": {
        "id": "s30F_jPyrfK_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "from transformers import YolosImageProcessor, YolosForObjectDetection\n",
        "from PIL import Image, ImageDraw\n",
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "# Ruta del video en tu equipo\n",
        "video_path = \"/content/pruebas.mp4\"\n",
        "\n",
        "# Cargar el modelo y el procesador de imágenes\n",
        "model = YolosForObjectDetection.from_pretrained('hustvl/yolos-tiny')\n",
        "image_processor = YolosImageProcessor.from_pretrained(\"hustvl/yolos-tiny\")\n",
        "\n",
        "# Crear el objeto VideoCapture para leer el video\n",
        "cap = cv2.VideoCapture(video_path)\n",
        "\n",
        "# Obtener el ancho y alto del video\n",
        "width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "\n",
        "# Crear el objeto VideoWriter para guardar el video con las detecciones\n",
        "output_path = \"/content/salida.mp4\"\n",
        "fourcc = cv2.VideoWriter_fourcc(*\"mp4v\")\n",
        "out = cv2.VideoWriter(output_path, fourcc, 30.0, (width, height))\n",
        "\n",
        "while cap.isOpened():\n",
        "    # Leer el siguiente frame del video\n",
        "    ret, frame = cap.read()\n",
        "\n",
        "    if not ret:\n",
        "        break\n",
        "\n",
        "    # Convertir el frame a formato PIL Image\n",
        "    image = Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
        "\n",
        "    # Procesar la imagen con el modelo de detección de objetos\n",
        "    inputs = image_processor(images=image, return_tensors=\"pt\")\n",
        "    outputs = model(**inputs)\n",
        "\n",
        "    # Obtener las detecciones de personas\n",
        "    target_sizes = torch.tensor([image.size[::-1]])\n",
        "    results = image_processor.post_process_object_detection(outputs, threshold=0.85, target_sizes=target_sizes)[0]\n",
        "\n",
        "    # Dibujar las cajas delimitadoras en el frame\n",
        "    draw = ImageDraw.Draw(image)\n",
        "    for score, label, box in zip(results[\"scores\"], results[\"labels\"], results[\"boxes\"]):\n",
        "        if model.config.id2label[label.item()] == \"person\":\n",
        "            box = [round(i, 2) for i in box.tolist()]\n",
        "            draw.rectangle(tuple(box), outline=\"red\", width=2)\n",
        "\n",
        "    # Guardar el frame con las detecciones en el video de salida\n",
        "    out.write(cv2.cvtColor(np.array(image), cv2.COLOR_RGB2BGR))\n",
        "\n",
        "# Liberar los recursos\n",
        "cap.release()\n",
        "out.release()\n",
        "cv2.destroyAllWindows()"
      ],
      "metadata": {
        "id": "6_ezbvGortqX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Escribimos el nombre persona sobre cada una de las personas y el factor de confianza"
      ],
      "metadata": {
        "id": "Kl9QRs3VBIIq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "from transformers import YolosImageProcessor, YolosForObjectDetection\n",
        "from PIL import Image, ImageDraw, ImageFont\n",
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "# Ruta del video en tu equipo\n",
        "video_path = \"/content/pruebas.mp4\"\n",
        "\n",
        "# Cargar el modelo y el procesador de imágenes\n",
        "model = YolosForObjectDetection.from_pretrained('hustvl/yolos-tiny')\n",
        "image_processor = YolosImageProcessor.from_pretrained(\"hustvl/yolos-tiny\")\n",
        "\n",
        "# Crear el objeto VideoCapture para leer el video\n",
        "cap = cv2.VideoCapture(video_path)\n",
        "\n",
        "# Obtener el ancho y alto del video\n",
        "width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "\n",
        "# Crear el objeto VideoWriter para guardar el video con las detecciones\n",
        "output_path = \"/content/salida.mp4\"\n",
        "fourcc = cv2.VideoWriter_fourcc(*\"mp4v\")\n",
        "out = cv2.VideoWriter(output_path, fourcc, 30.0, (width, height))\n",
        "\n",
        "# Fuente para el texto\n",
        "font = ImageFont.truetype(\"/content/Arial.ttf\", 14)\n",
        "\n",
        "while cap.isOpened():\n",
        "    # Leer el siguiente frame del video\n",
        "    ret, frame = cap.read()\n",
        "\n",
        "    if not ret:\n",
        "        break\n",
        "\n",
        "    # Convertir el frame a formato PIL Image\n",
        "    image = Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
        "\n",
        "    # Procesar la imagen con el modelo de detección de objetos\n",
        "    inputs = image_processor(images=image, return_tensors=\"pt\")\n",
        "    outputs = model(**inputs)\n",
        "\n",
        "    # Obtener las detecciones de personas\n",
        "    target_sizes = torch.tensor([image.size[::-1]])\n",
        "    results = image_processor.post_process_object_detection(outputs, threshold=0.85, target_sizes=target_sizes)[0]\n",
        "\n",
        "    # Dibujar las cajas delimitadoras en el frame\n",
        "    draw = ImageDraw.Draw(image)\n",
        "    for score, label, box in zip(results[\"scores\"], results[\"labels\"], results[\"boxes\"]):\n",
        "        if model.config.id2label[label.item()] == \"person\":\n",
        "            box = [round(i, 2) for i in box.tolist()]\n",
        "            draw.rectangle(tuple(box), outline=\"red\", width=2)\n",
        "\n",
        "            # Obtener el valor de confianza\n",
        "            confidence = round(score.item(), 2)\n",
        "\n",
        "            # Agregar el texto \"persona\" y el valor de confianza sobre el cuadro delimitador\n",
        "            text = f\"Persona: {confidence}\"\n",
        "            # Obtener el tamaño del texto\n",
        "            text_bbox = draw.textbbox(text_position, text, font=font)\n",
        "            text_width = text_bbox[2] - text_bbox[0]\n",
        "            text_height = text_bbox[3] - text_bbox[1]\n",
        "            text_position = (box[0], box[1] - text_height - 5)\n",
        "            draw.rectangle([text_position, (text_position[0] + text_width, text_position[1] + text_height)], fill=\"red\")\n",
        "            draw.text(text_position, text, fill=\"white\", font=font)\n",
        "\n",
        "    # Guardar el frame con las detecciones en el video de salida\n",
        "    out.write(cv2.cvtColor(np.array(image), cv2.COLOR_RGB2BGR))\n",
        "\n",
        "# Liberar los recursos\n",
        "cap.release()\n",
        "out.release()\n",
        "cv2.destroyAllWindows()"
      ],
      "metadata": {
        "id": "0zjlnJsmBOCP"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Vamos a probar ahora a reconocer a cada jugador por separado\n",
        "Intento fallido"
      ],
      "metadata": {
        "id": "UromLC8sjBEs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "from transformers import YolosImageProcessor, YolosForObjectDetection\n",
        "from PIL import Image, ImageDraw, ImageFont\n",
        "import torch\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "# Ruta del video en tu equipo\n",
        "video_path = \"/content/pruebas.mp4\"\n",
        "\n",
        "# Cargar el modelo y el procesador de imágenes\n",
        "model = YolosForObjectDetection.from_pretrained('hustvl/yolos-tiny')\n",
        "image_processor = YolosImageProcessor.from_pretrained(\"hustvl/yolos-tiny\")\n",
        "\n",
        "# Crear el objeto VideoCapture para leer el video\n",
        "cap = cv2.VideoCapture(video_path)\n",
        "\n",
        "# Obtener el ancho y alto del video\n",
        "width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "\n",
        "# Crear el objeto VideoWriter para guardar el video con las detecciones\n",
        "output_path = \"/content/salida.mp4\"\n",
        "fourcc = cv2.VideoWriter_fourcc(*\"mp4v\")\n",
        "out = cv2.VideoWriter(output_path, fourcc, 30.0, (width, height))\n",
        "\n",
        "# Fuente para el texto\n",
        "font = ImageFont.truetype(\"/content/Arial.ttf\", 14)\n",
        "\n",
        "# Diccionario para almacenar los colores de los jugadores\n",
        "player_colors = {}\n",
        "\n",
        "while cap.isOpened():\n",
        "    # Leer el siguiente frame del video\n",
        "    ret, frame = cap.read()\n",
        "\n",
        "    if not ret:\n",
        "        break\n",
        "\n",
        "    # Convertir el frame a formato PIL Image\n",
        "    image = Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
        "\n",
        "    # Procesar la imagen con el modelo de detección de objetos\n",
        "    inputs = image_processor(images=image, return_tensors=\"pt\")\n",
        "    outputs = model(**inputs)\n",
        "\n",
        "    # Obtener las detecciones de personas\n",
        "    target_sizes = torch.tensor([image.size[::-1]])\n",
        "    results = image_processor.post_process_object_detection(outputs, threshold=0.85, target_sizes=target_sizes)[0]\n",
        "\n",
        "    # Dibujar las cajas delimitadoras en el frame\n",
        "    draw = ImageDraw.Draw(image)\n",
        "    for score, label, box in zip(results[\"scores\"], results[\"labels\"], results[\"boxes\"]):\n",
        "        if model.config.id2label[label.item()] == \"person\":\n",
        "            box = [round(i, 2) for i in box.tolist()]\n",
        "\n",
        "            # Asignar un color al jugador si no está asignado\n",
        "            if tuple(box) not in player_colors:\n",
        "                player_colors[tuple(box)] = tuple(random.randint(0, 255) for _ in range(3))\n",
        "\n",
        "            # Dibujar el rectángulo con el color asignado\n",
        "            draw.rectangle(tuple(box), outline=player_colors[tuple(box)], width=2)\n",
        "\n",
        "            # Obtener el valor de confianza\n",
        "            confidence = round(score.item(), 2)\n",
        "\n",
        "            # Agregar el texto \"persona\" y el valor de confianza sobre el cuadro delimitador\n",
        "            text = f\"Persona: {confidence}\"\n",
        "            # Obtener el tamaño del texto\n",
        "            text_bbox = draw.textbbox((box[0], box[1] - 15), text, font=font)\n",
        "            text_width = text_bbox[2] - text_bbox[0]\n",
        "            text_height = text_bbox[3] - text_bbox[1]\n",
        "            text_position = (box[0], box[1] - text_height - 5)\n",
        "            draw.rectangle([text_position, (text_position[0] + text_width, text_position[1] + text_height)], fill=player_colors[tuple(box)])\n",
        "            draw.text(text_position, text, fill=\"white\", font=font)\n",
        "\n",
        "    # Guardar el frame con las detecciones en el video de salida\n",
        "    out.write(cv2.cvtColor(np.array(image), cv2.COLOR_RGB2BGR))\n",
        "\n",
        "# Liberar los recursos\n",
        "cap.release()\n",
        "out.release()\n",
        "cv2.destroyAllWindows()"
      ],
      "metadata": {
        "id": "mMExuxogj0DF"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ¿Y si tratamos de hacer que los elementos detectados de la mitad de la imágen hacia la izquierda sean de un color y el resto de otro?"
      ],
      "metadata": {
        "id": "d-zkSUU3qo67"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "from transformers import YolosImageProcessor, YolosForObjectDetection\n",
        "from PIL import Image, ImageDraw, ImageFont\n",
        "import torch\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "# Ruta del video en tu equipo\n",
        "video_path = \"/content/pruebas.mp4\"\n",
        "\n",
        "# Cargar el modelo y el procesador de imágenes\n",
        "model = YolosForObjectDetection.from_pretrained('hustvl/yolos-tiny')\n",
        "image_processor = YolosImageProcessor.from_pretrained(\"hustvl/yolos-tiny\")\n",
        "\n",
        "# Crear el objeto VideoCapture para leer el video\n",
        "cap = cv2.VideoCapture(video_path)\n",
        "\n",
        "# Obtener el ancho y alto del video\n",
        "width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "\n",
        "# Crear el objeto VideoWriter para guardar el video con las detecciones\n",
        "output_path = \"/content/salida.mp4\"\n",
        "fourcc = cv2.VideoWriter_fourcc(*\"mp4v\")\n",
        "out = cv2.VideoWriter(output_path, fourcc, 30.0, (width, height))\n",
        "\n",
        "# Fuente para el texto\n",
        "font = ImageFont.truetype(\"/content/Arial.ttf\", 14)\n",
        "\n",
        "# Diccionario para almacenar los colores de los jugadores\n",
        "player_colors = {}\n",
        "\n",
        "while cap.isOpened():\n",
        "    # Leer el siguiente frame del video\n",
        "    ret, frame = cap.read()\n",
        "\n",
        "    if not ret:\n",
        "        break\n",
        "\n",
        "    # Convertir el frame a formato PIL Image\n",
        "    image = Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
        "\n",
        "    # Procesar la imagen con el modelo de detección de objetos\n",
        "    inputs = image_processor(images=image, return_tensors=\"pt\")\n",
        "    outputs = model(**inputs)\n",
        "\n",
        "    # Obtener las detecciones de personas\n",
        "    target_sizes = torch.tensor([image.size[::-1]])\n",
        "    results = image_processor.post_process_object_detection(outputs, threshold=0.85, target_sizes=target_sizes)[0]\n",
        "\n",
        "    # Dibujar las cajas delimitadoras en el frame\n",
        "    draw = ImageDraw.Draw(image)\n",
        "    for score, label, box in zip(results[\"scores\"], results[\"labels\"], results[\"boxes\"]):\n",
        "        if model.config.id2label[label.item()] == \"person\":\n",
        "            box = [round(i, 2) for i in box.tolist()]\n",
        "\n",
        "            # Asignar un color al jugador si no está asignado\n",
        "            if tuple(box) not in player_colors:\n",
        "                # Obtener la posición x del centro de la caja delimitadora\n",
        "                center_x = (box[0] + box[2]) / 2\n",
        "\n",
        "                # Asignar un color basado en la posición x\n",
        "                if center_x <= width / 2:\n",
        "                    player_colors[tuple(box)] = (255, 0, 0)  # Rojo\n",
        "                else:\n",
        "                    player_colors[tuple(box)] = (0, 255, 0)  # Verde\n",
        "\n",
        "            # Dibujar el rectángulo con el color asignado\n",
        "            draw.rectangle(tuple(box), outline=player_colors[tuple(box)], width=2)\n",
        "\n",
        "            # Obtener el valor de confianza\n",
        "            confidence = round(score.item(), 2)\n",
        "\n",
        "            # Agregar el texto \"persona\" y el valor de confianza sobre el cuadro delimitador\n",
        "            text = f\"Persona: {confidence}\"\n",
        "            # Obtener el tamaño del texto\n",
        "            text_bbox = draw.textbbox((box[0], box[1] - 15), text, font=font)\n",
        "            text_width = text_bbox[2] - text_bbox[0]\n",
        "            text_height = text_bbox[3] - text_bbox[1]\n",
        "            text_position = (box[0], box[1] - text_height - 5)\n",
        "            draw.rectangle([text_position, (text_position[0] + text_width, text_position[1] + text_height)],\n",
        "                           fill=player_colors[tuple(box)])\n",
        "            draw.text(text_position, text, fill=\"white\", font=font)\n",
        "\n",
        "    # Guardar el frame con las detecciones en el video de salida\n",
        "    out.write(cv2.cvtColor(np.array(image), cv2.COLOR_RGB2BGR))\n",
        "\n",
        "# Liberar los recursos\n",
        "cap.release()\n",
        "out.release()\n",
        "cv2.destroyAllWindows()\n"
      ],
      "metadata": {
        "id": "5UBlZJPgq0Tq"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ¿Y si detectamos primero la red y despues coloreamos los cuadrados en función a la posicion de la red?\n",
        "\n",
        "Se nos acaba toda la RAM"
      ],
      "metadata": {
        "id": "wIyyWtC80QBb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "from transformers import YolosImageProcessor, YolosForObjectDetection\n",
        "from PIL import Image, ImageDraw, ImageFont\n",
        "import torch\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "# Ruta del video en tu equipo\n",
        "video_path = \"/content/pruebas.mp4\"\n",
        "\n",
        "# Cargar el modelo y el procesador de imágenes\n",
        "model = YolosForObjectDetection.from_pretrained('hustvl/yolos-tiny')\n",
        "image_processor = YolosImageProcessor.from_pretrained(\"hustvl/yolos-tiny\")\n",
        "\n",
        "# Crear el objeto VideoCapture para leer el video\n",
        "cap = cv2.VideoCapture(video_path)\n",
        "\n",
        "# Obtener el ancho y alto del video\n",
        "width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "\n",
        "# Crear el objeto VideoWriter para guardar el video con las detecciones\n",
        "output_path = \"/content/salida.mp4\"\n",
        "fourcc = cv2.VideoWriter_fourcc(*\"mp4v\")\n",
        "out = cv2.VideoWriter(output_path, fourcc, 30.0, (width, height))\n",
        "\n",
        "# Fuente para el texto\n",
        "font = ImageFont.truetype(\"/content/Arial.ttf\", 14)\n",
        "\n",
        "# Diccionario para almacenar los colores de los jugadores\n",
        "player_colors = {}\n",
        "\n",
        "while cap.isOpened():\n",
        "    # Leer el siguiente frame del video\n",
        "    ret, frame = cap.read()\n",
        "\n",
        "    if not ret:\n",
        "        break\n",
        "\n",
        "    # Convertir el frame a formato PIL Image\n",
        "    image = Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
        "\n",
        "    # Procesar la imagen con el modelo de detección de objetos\n",
        "    inputs = image_processor(images=image, return_tensors=\"pt\")\n",
        "    outputs = model(**inputs)\n",
        "\n",
        "    # Obtener las detecciones de personas y raquetas de tenis\n",
        "    target_sizes = torch.tensor([image.size[::-1]])\n",
        "    results = image_processor.post_process_object_detection(outputs, threshold=0.85, target_sizes=target_sizes)[0]\n",
        "\n",
        "    # Dibujar las cajas delimitadoras en el frame\n",
        "    draw = ImageDraw.Draw(image)\n",
        "    for score, label, box in zip(results[\"scores\"], results[\"labels\"], results[\"boxes\"]):\n",
        "        if model.config.id2label[label.item()] == \"person\":\n",
        "            box = [round(i, 2) for i in box.tolist()]\n",
        "\n",
        "            # Obtener la posición x del centro de la caja delimitadora\n",
        "            center_x = (box[0] + box[2]) / 2\n",
        "\n",
        "            # Buscar la raqueta de tenis más cercana\n",
        "            tennis_racket_box = None\n",
        "            for score, label, box in zip(results[\"scores\"], results[\"labels\"], results[\"boxes\"]):\n",
        "                if model.config.id2label[label.item()] == \"tennis racket\":\n",
        "                    tennis_racket_box = box\n",
        "                    break\n",
        "\n",
        "            # Si se encuentra una raqueta de tenis, colorear al jugador según su posición\n",
        "            if tennis_racket_box is not None:\n",
        "                # Obtener la posición x del centro de la raqueta de tenis\n",
        "                tennis_racket_center_x = (tennis_racket_box[0] + tennis_racket_box[2]) / 2\n",
        "\n",
        "                # Asignar color según la posición relativa a la raqueta\n",
        "                if center_x < tennis_racket_center_x:\n",
        "                    player_colors[tuple(box)] = (255, 0, 0)  # Rojo (izquierda)\n",
        "                else:\n",
        "                    player_colors[tuple(box)] = (0, 255, 0)  # Verde (derecha)\n",
        "\n",
        "            # Dibujar el rectángulo con el color asignado\n",
        "            color = player_colors.get(tuple(box), (0, 0, 0))  # Default color is black if key is not found\n",
        "            draw.rectangle(tuple(box), outline=color, width=2)\n",
        "\n",
        "            # Obtener el valor de confianza\n",
        "            confidence = round(score.item(), 2)\n",
        "\n",
        "            # Agregar el texto \"persona\" y el valor de confianza sobre el cuadro delimitador\n",
        "            text = f\"Persona: {confidence}\"\n",
        "            # Obtener el tamaño del texto\n",
        "            text_bbox = draw.textbbox((box[0], box[1] - 15), text, font=font)\n",
        "            text_width = text_bbox[2] - text_bbox[0]\n",
        "            text_height = text_bbox[3] - text_bbox[1]\n",
        "            text_position = (box[0], box[1] - text_height - 5)\n",
        "            #draw.rectangle([text_position, (text_position[0] + text_width, text_position[1] + text_height)], fill=player_colors[tuple(box)])\n",
        "            draw.text(text_position, text, fill=\"white\", font=font)\n",
        "\n",
        "    # Guardar el frame con las detecciones en el video de salida\n",
        "    out.write(cv2.cvtColor(np.array(image), cv2.COLOR_RGB2BGR))\n",
        "\n",
        "# Liberar los recursos\n",
        "cap.release()\n",
        "out.release()\n",
        "cv2.destroyAllWindows()\n"
      ],
      "metadata": {
        "id": "z36cKusV0af1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Probamos con GPU\n",
        "No funciona porque está mal optimizado"
      ],
      "metadata": {
        "id": "BRdZ-a7k7nIq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "from transformers import YolosImageProcessor, YolosForObjectDetection\n",
        "from PIL import Image, ImageDraw, ImageFont\n",
        "import torch\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "# Verificar si hay una GPU disponible\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Ruta del video en tu equipo\n",
        "video_path = \"/content/pruebas.mp4\"\n",
        "\n",
        "# Cargar el modelo y el procesador de imágenes y mover el modelo a la GPU\n",
        "model = YolosForObjectDetection.from_pretrained('hustvl/yolos-tiny').to(device)\n",
        "image_processor = YolosImageProcessor.from_pretrained(\"hustvl/yolos-tiny\")\n",
        "\n",
        "# Crear el objeto VideoCapture para leer el video\n",
        "cap = cv2.VideoCapture(video_path)\n",
        "\n",
        "# Obtener el ancho y alto del video\n",
        "width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "\n",
        "# Crear el objeto VideoWriter para guardar el video con las detecciones\n",
        "output_path = \"/content/salida.mp4\"\n",
        "fourcc = cv2.VideoWriter_fourcc(*\"mp4v\")\n",
        "out = cv2.VideoWriter(output_path, fourcc, 30.0, (width, height))\n",
        "\n",
        "# Fuente para el texto\n",
        "font = ImageFont.truetype(\"/content/Arial.ttf\", 14)\n",
        "\n",
        "# Diccionario para almacenar los colores de los jugadores\n",
        "player_colors = {}\n",
        "\n",
        "while cap.isOpened():\n",
        "    # Leer el siguiente frame del video\n",
        "    ret, frame = cap.read()\n",
        "\n",
        "    if not ret:\n",
        "        break\n",
        "\n",
        "    # Convertir el frame a formato PIL Image\n",
        "    image = Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
        "\n",
        "    # Procesar la imagen con el modelo de detección de objetos\n",
        "    inputs = image_processor(images=image, return_tensors=\"pt\").to(device)\n",
        "    outputs = model(**inputs)\n",
        "\n",
        "    # Obtener las detecciones de personas y raquetas de tenis\n",
        "    target_sizes = torch.tensor([image.size[::-1]]).to(device)\n",
        "    results = image_processor.post_process_object_detection(outputs, threshold=0.85, target_sizes=target_sizes)[0]\n",
        "\n",
        "    # Dibujar las cajas delimitadoras en el frame\n",
        "    draw = ImageDraw.Draw(image)\n",
        "    for score, label, box in zip(results[\"scores\"], results[\"labels\"], results[\"boxes\"]):\n",
        "        if model.config.id2label[label.item()] == \"person\":\n",
        "            box = [round(i, 2) for i in box.tolist()]\n",
        "\n",
        "            # Obtener la posición x del centro de la caja delimitadora\n",
        "            center_x = (box[0] + box[2]) / 2\n",
        "\n",
        "            # Buscar la raqueta de tenis más cercana\n",
        "            tennis_racket_box = None\n",
        "            for score, label, box in zip(results[\"scores\"], results[\"labels\"], results[\"boxes\"]):\n",
        "                if model.config.id2label[label.item()] == \"tennis racket\":\n",
        "                    tennis_racket_box = box\n",
        "                    break\n",
        "\n",
        "            # Si se encuentra una raqueta de tenis, colorear al jugador según su posición\n",
        "            if tennis_racket_box is not None:\n",
        "                # Obtener la posición x del centro de la raqueta de tenis\n",
        "                tennis_racket_center_x = (tennis_racket_box[0] + tennis_racket_box[2]) / 2\n",
        "\n",
        "                # Asignar color según la posición relativa a la raqueta\n",
        "                if center_x < tennis_racket_center_x:\n",
        "                    player_colors[tuple(box)] = (255, 0, 0)  # Rojo (izquierda)\n",
        "                else:\n",
        "                    player_colors[tuple(box)] = (0, 255, 0)  # Verde (derecha)\n",
        "\n",
        "            # Dibujar el rectángulo con el color asignado\n",
        "            color = player_colors.get(tuple(box), (0, 0, 0))  # Default color is black if key is not found\n",
        "            draw.rectangle(tuple(box), outline=color, width=2)\n",
        "\n",
        "            # Obtener el valor de confianza\n",
        "            confidence = round(score.item(), 2)\n",
        "\n",
        "            # Agregar el texto \"persona\" y el valor de confianza sobre el cuadro delimitador\n",
        "            text = f\"Persona: {confidence}\"\n",
        "            # Obtener el tamaño del texto\n",
        "            text_bbox = draw.textbbox((box[0], box[1] - 15), text, font=font)\n",
        "            text_width = text_bbox[2] - text_bbox[0]\n",
        "            text_height = text_bbox[3] - text_bbox[1]\n",
        "            text_position = (box[0], box[1] - text_height - 5)\n",
        "            #draw.rectangle([text_position, (text_position[0] + text_width, text_position[1] + text_height)], fill=player_colors[tuple(box)])\n",
        "            draw.text(text_position, text, fill=\"white\", font=font)\n",
        "\n",
        "    # Guardar el frame con las detecciones en el video de salida\n",
        "    out.write(cv2.cvtColor(np.array(image), cv2.COLOR_RGB2BGR))\n",
        "\n",
        "# Liberar los recursos\n",
        "cap.release()\n",
        "out.release()\n",
        "cv2.destroyAllWindows()"
      ],
      "metadata": {
        "id": "uAVWTqnp7qdQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Optimizamos ahora el código y hacemos trabajar a la GPU"
      ],
      "metadata": {
        "id": "Ay6RhMsn9sOG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "from transformers import YolosImageProcessor, YolosForObjectDetection\n",
        "from PIL import Image, ImageDraw, ImageFont\n",
        "import torch\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "# Verificar si hay una GPU disponible\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Ruta del video en tu equipo\n",
        "video_path = \"/content/pruebas.mp4\"\n",
        "\n",
        "# Cargar el modelo y el procesador de imágenes y mover el modelo a la GPU\n",
        "model = YolosForObjectDetection.from_pretrained('hustvl/yolos-tiny').to(device)\n",
        "image_processor = YolosImageProcessor.from_pretrained(\"hustvl/yolos-tiny\")\n",
        "\n",
        "# Crear el objeto VideoCapture para leer el video\n",
        "cap = cv2.VideoCapture(video_path)\n",
        "\n",
        "# Obtener el ancho y alto del video\n",
        "width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "\n",
        "# Crear el objeto VideoWriter para guardar el video con las detecciones\n",
        "output_path = \"/content/salida.mp4\"\n",
        "fourcc = cv2.VideoWriter_fourcc(*\"mp4v\")\n",
        "out = cv2.VideoWriter(output_path, fourcc, 30.0, (width, height))\n",
        "\n",
        "# Fuente para el texto\n",
        "font = ImageFont.truetype(\"/content/Arial.ttf\", 14)\n",
        "\n",
        "# Diccionario para almacenar los colores de los jugadores\n",
        "player_colors = {}\n",
        "\n",
        "while cap.isOpened():\n",
        "    # Leer el siguiente frame del video\n",
        "    ret, frame = cap.read()\n",
        "\n",
        "    if not ret:\n",
        "        break\n",
        "\n",
        "    # Convertir el frame a formato PIL Image\n",
        "    image = Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
        "\n",
        "    with torch.no_grad():\n",
        "        # Procesar la imagen con el modelo de detección de objetos\n",
        "        inputs = image_processor(images=image, return_tensors=\"pt\").to(device)\n",
        "        outputs = model(**inputs)\n",
        "\n",
        "        # Obtener las detecciones de personas y raquetas de tenis\n",
        "        target_sizes = torch.tensor([image.size[::-1]]).to(device)\n",
        "        results = image_processor.post_process_object_detection(outputs, threshold=0.85, target_sizes=target_sizes)[0]\n",
        "\n",
        "    # Dibujar las cajas delimitadoras en el frame\n",
        "    draw = ImageDraw.Draw(image)\n",
        "    for score, label, box in zip(results[\"scores\"], results[\"labels\"], results[\"boxes\"]):\n",
        "        if model.config.id2label[label.item()] == \"person\":\n",
        "            box = [round(i, 2) for i in box.tolist()]\n",
        "\n",
        "            # Obtener la posición x del centro de la caja delimitadora\n",
        "            center_x = (box[0] + box[2]) / 2\n",
        "\n",
        "            # Buscar la raqueta de tenis más cercana\n",
        "            tennis_racket_box = None\n",
        "            for racket_score, racket_label, racket_box in zip(results[\"scores\"], results[\"labels\"], results[\"boxes\"]):\n",
        "                if model.config.id2label[racket_label.item()] == \"tennis racket\":\n",
        "                    tennis_racket_box = racket_box\n",
        "                    break\n",
        "\n",
        "            # Si se encuentra una raqueta de tenis, colorear al jugador según su posición\n",
        "            if tennis_racket_box is not None:\n",
        "                # Obtener la posición x del centro de la raqueta de tenis\n",
        "                tennis_racket_center_x = (tennis_racket_box[0] + tennis_racket_box[2]) / 2\n",
        "\n",
        "                # Asignar color según la posición relativa a la raqueta\n",
        "                if center_x < tennis_racket_center_x:\n",
        "                    player_colors[tuple(box)] = (255, 0, 0)  # Rojo (izquierda)\n",
        "                else:\n",
        "                    player_colors[tuple(box)] = (0, 255, 0)  # Verde (derecha)\n",
        "\n",
        "            # Dibujar el rectángulo con el color asignado\n",
        "            color = player_colors.get(tuple(box), (0, 0, 0))  # Default color is black if key is not found\n",
        "            draw.rectangle(tuple(box), outline=color, width=2)\n",
        "\n",
        "            # Obtener el valor de confianza\n",
        "            confidence = round(score.item(), 2)\n",
        "\n",
        "            # Agregar el texto \"persona\" y el valor de confianza sobre el cuadro delimitador\n",
        "            text = f\"Persona: {confidence}\"\n",
        "            # Obtener el tamaño del texto\n",
        "            text_bbox = draw.textbbox((box[0], box[1] - 15), text, font=font)\n",
        "            text_width = text_bbox[2] - text_bbox[0]\n",
        "            text_height = text_bbox[3] - text_bbox[1]\n",
        "            text_position = (box[0], box[1] - text_height - 5)\n",
        "            draw.text(text_position, text, fill=\"white\", font=font)\n",
        "\n",
        "    # Guardar el frame con las detecciones en el video de salida\n",
        "    out.write(cv2.cvtColor(np.array(image), cv2.COLOR_RGB2BGR))\n",
        "\n",
        "    # Liberar memoria de tensores no utilizados\n",
        "    del inputs, outputs, target_sizes, results\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "# Liberar los recursos\n",
        "cap.release()\n",
        "out.release()\n",
        "cv2.destroyAllWindows()"
      ],
      "metadata": {
        "id": "L95qkpO99rsJ"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Ya hemos conseguido nuestro objetivo"
      ],
      "metadata": {
        "id": "RC3iq_I7Mutz"
      }
    }
  ]
}